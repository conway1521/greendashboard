{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13c6d3e2-45ee-4b03-925c-48d06b8a2e6b",
   "metadata": {},
   "source": [
    "# Green Job Transformation data\n",
    "\n",
    "1. Collect and clean data\n",
    "2. Remove outliers and prepare for Factor Analysis\n",
    "3. Principal Component Analysis to get weights\n",
    "4. Standardise full dataset to prepare for assigning weights and aggregate; Assign weights to calculate Risk/Readiness\n",
    "6. Calculate Risk/Readiness\n",
    "7. Calculate Type\n",
    "8. Export dataset for graphing/mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e2ee748-9e27-41c2-ae59-18481f444bc4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Packages installed and imported\n"
     ]
    }
   ],
   "source": [
    "#!conda install conda=23.3.1 --yes\n",
    "\n",
    "# import sys;\n",
    "# !conda install --yes --prefix {sys.prefix} plotly;\n",
    "# !conda install --yes --prefix {sys.prefix} pandas;\n",
    "# !conda install --yes --prefix {sys.prefix} matplotlib;\n",
    "# !conda install --yes --prefix {sys.prefix} numpy;\n",
    "# !conda install --yes --prefix {sys.prefix} openpyxl;\n",
    "# !conda install --yes --prefix {sys.prefix} pyjanitor;\n",
    "# #!conda install --yes --prefix {sys.prefix} dash_bootstrap_components\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import MinMaxScaler;\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "print(\"Packages installed and imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235e412e-d7fc-4f3e-8017-7e571838634b",
   "metadata": {},
   "source": [
    "## Collect and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ce4c0af-4dab-457f-b486-1f4b617d8fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gini = pd.read_csv('/Users/aconway/Desktop/CREST/Crest_data/gini_income.csv', encoding='latin-1')\n",
    "\n",
    "# County and Fips codes\n",
    "df_countyFIPSall = pd.read_csv('/Users/aconway/Desktop/CREST/Crest_data/countyFIPS.csv', encoding='UTF-8')\n",
    "df_countyFIPS = df_countyFIPSall.iloc[:, 0:1]\n",
    "\n",
    "# Social Vulnerability data\n",
    "df_svi = pd.read_csv('/Users/aconway/Desktop/CREST/Crest_data/svi_interactive_map.csv', encoding='latin-1')\n",
    "df_svi = df_svi.drop(columns=['COUNTY'])\n",
    "\n",
    "# Weather patterns data\n",
    "df_nri = pd.read_csv('/Users/aconway/Desktop/CREST/Crest_data/NRI_Table_counties.csv', encoding='latin-1')\n",
    "df_nri = df_nri.drop(columns=['COUNTY'])\n",
    "\n",
    "# Politics data\n",
    "df_polclime = pd.read_csv('/Users/aconway/Desktop/CREST/Crest_data/politicalclimate2021.csv', encoding='latin-1')\n",
    "df_polclime.columns.values[0] = \"CountyFIPS\"\n",
    "df_polclime = df_polclime.drop(columns=['congressmore'])\n",
    "\n",
    "df_statepol = pd.read_csv('/Users/aconway/Desktop/CREST/Crest_data/state_policy.csv', encoding='UTF-8')\n",
    "df_statepol = df_statepol[[\"CountyFIPS\", \"state_policy\"]]\n",
    "\n",
    "df_polclime = pd.merge(df_polclime, df_statepol, on='CountyFIPS', how='left')\n",
    "\n",
    "# #merging inner to keep only common counties\n",
    "df_first = pd.merge(df_nri, df_svi, on='CountyFIPS', how='inner')\n",
    "df_tot = pd.merge(df_first, df_polclime, on='CountyFIPS', how='inner')\n",
    "\n",
    "dfu = pd.read_csv(\"https://raw.githubusercontent.com/plotly/datasets/master/fips-unemp-16.csv\")  # ,\n",
    "# dtype={\"fips\": str})\n",
    "dfu = dfu.rename(columns={\"fips\": \"CountyFIPS\"})\n",
    "\n",
    "df_tot = pd.merge(df_tot, dfu, on=\"CountyFIPS\", how=\"inner\")\n",
    "\n",
    "# df_countyFIPS = df_tot.iloc[:,0:1]\n",
    "\n",
    "\n",
    "# LM data\n",
    "df_lm = pd.read_csv('/Users/aconway/Desktop/CREST/Crest_data/green_skills.csv', encoding='UTF-8')\n",
    "df_lm_cap = pd.merge(df_countyFIPS, df_lm, on=\"CountyFIPS\", how=\"inner\")\n",
    "df_lm = pd.merge(df_countyFIPS, df_lm, on=\"CountyFIPS\", how=\"left\")\n",
    "\n",
    "interp_lm = [0.6136, 7.323, -0.847, -0.046]\n",
    "nan_values = df_lm[df_lm.isna().any(axis=1)]\n",
    "\n",
    "for row, index in nan_values.iterrows():\n",
    "    nan_values.loc[row, \"green_skill_share\"] = interp_lm[0]\n",
    "    nan_values.loc[row, \"green_skill_rate\"] = interp_lm[1]\n",
    "    nan_values.loc[row, \"rate_change_since_2018\"] = interp_lm[2]\n",
    "    nan_values.loc[row, \"share_change_since_2018\"] = interp_lm[3]\n",
    "\n",
    "df_lm.update(nan_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "553299f7-d484-4d0c-929f-74d0a339c437",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "#if need to aggregate across different types, do normalisation\n",
    "\n",
    "#climate layer\n",
    "df['clim_freq_log'] = np.log(df_tot.filter(regex='freq').mean(axis=1, numeric_only=True).round(3) + 1) ###### ADD 1 because have values <1, so get negative from log\n",
    "df['clim_risk'] = df_tot.filter(regex='risk').drop(columns='risk_score').drop(columns='community_riskfactor_score').mean(axis=1, numeric_only=True).round(3)\n",
    "df['clim_exposure_log'] = np.log(df_tot.filter(regex='exposure').mean(axis=1, numeric_only=True).round(3))\n",
    "df['clim_lossannual'] = df_tot.filter(regex='loss').drop(columns='exp_annual_loss_score').mean(axis=1, numeric_only=True).round(3)\n",
    "\n",
    "#political sentiment layer\n",
    "df['pol_localofficialsmore'] = df_tot.filter(items=['localofficialsmore']).round(3)\n",
    "df['pol_statepolicy'] = df_tot.filter(items=['state_policy']).round(3)\n",
    "df['pol_bussentiment'] = df_tot.filter(items=['corporationscause']).round(3)\n",
    "df['pol_gweffect'] = df_tot.filter(items=['experiencegw']).round(3)\n",
    "\n",
    "#social vulnerability layer\n",
    "df_tot['community_riskfactor']=(100-df_tot['community_riskfactor_score'])\n",
    "df['sv_community'] = df_tot.filter(items=['community_resilience', 'community_riskfactor']).mean(axis=1, numeric_only=True).round(3)\n",
    "df['sv_resilience'] = df_tot.filter(items=['below_pov150', 'housingcost_burden']).mean(axis=1, numeric_only=True).round(3)\n",
    "df['sv_marginalisation'] = df_tot.filter(items=['disabled', 'singleparenthh', 'minority', 'limited_english']).mean(axis=1, numeric_only=True).round(3)\n",
    "df['sv_vulnerability'] = df_tot.filter(items=['no_hsdiploma', 'uninsured', 'unemp']).mean(axis=1, numeric_only=True).round(3) ###unemployed?\n",
    "\n",
    "#add County and fips codes\n",
    "df = pd.concat([df_countyFIPS,df],axis=1,join='inner')\n",
    "\n",
    "# Remove rows with NaN values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Reset the row index\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9f89c4-1589-422e-a16e-4919aacaab1d",
   "metadata": {},
   "source": [
    "## Remove outliers and prepare for Factor Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5204a6d-309b-46d3-82b9-e3f22d052c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_outliers_IQR(df):\n",
    "   q1=df.quantile(0.25)\n",
    "   q3=df.quantile(0.75)\n",
    "   IQR=q3-q1\n",
    "\n",
    "   outliers = df[((df<(q1-1.5*IQR)) | (df>(q3+1.5*IQR)))]\n",
    "\n",
    "   return outliers\n",
    "\n",
    "# Create the dataset for capping\n",
    "df_cap = df.copy()\n",
    "\n",
    "# Fill cap dataset by removing outliers\n",
    "for col in df_cap.columns[1:]:\n",
    "    var = col\n",
    "    upper_limit = df_cap[var].mean() + 3 * df_cap[var].std()\n",
    "    lower_limit = df_cap[var].mean() - 3 * df_cap[var].std()\n",
    "\n",
    "    if lower_limit < 0:\n",
    "        lower_limit = 0\n",
    "    outliers = find_outliers_IQR(df[var])\n",
    "\n",
    "    df_cap[var] = np.where(df_cap[var] > upper_limit, np.nan, df_cap[var])\n",
    "    df_cap[var] = np.where(df_cap[var] < lower_limit, np.nan, df_cap[var])\n",
    "\n",
    "# Remove df_cap rows with NaN values\n",
    "df_cap.dropna(inplace=True)\n",
    "\n",
    "# Reset the row index\n",
    "df_cap.reset_index(drop=True, inplace=True)\n",
    "#-----\n",
    "\n",
    "# Add LM layer (with the ALI addition, already cleaned of outliers, and match on inner counties) to df and df_cap\n",
    "\n",
    "# labor market layer\n",
    "lm_prevalence =  df_lm.iloc[:, [0,1]]\n",
    "lm_demand =  df_lm.iloc[:, [0,2]]\n",
    "lm_demand_growth =  df_lm.iloc[:, [0,3]]\n",
    "lm_prevalence_growth =  df_lm.iloc[:, [0,4]]\n",
    "\n",
    "df_lm.columns = ['CountyFIPS', 'lm_prevalence', 'lm_demand', 'lm_demand_growth', 'lm_prevalence_growth']\n",
    "df_lm.dropna(inplace=True)\n",
    "df_lm.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df = pd.merge(df, df_lm, on = \"CountyFIPS\", how = \"inner\")\n",
    "df = pd.merge(df_countyFIPSall, df, on = \"CountyFIPS\", how = \"inner\")\n",
    "\n",
    "#do the same for the CAP dataset\n",
    "lm_prevalence_cap =  df_lm_cap.iloc[:, [0,1]]\n",
    "lm_demand_cap =  df_lm_cap.iloc[:, [0,2]]\n",
    "lm_demand_growth_cap =  df_lm_cap.iloc[:, [0,3]]\n",
    "lm_prevalence_growth_cap =  df_lm_cap.iloc[:, [0,4]]\n",
    "\n",
    "df_lm_cap.columns = ['CountyFIPS', 'lm_prevalence_cap', 'lm_demand_cap', 'lm_demand_growth_cap', 'lm_prevalence_growth_cap']\n",
    "df_lm_cap.dropna(inplace=True)\n",
    "df_lm_cap.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_cap = pd.merge(df_cap, df_lm_cap, on = \"CountyFIPS\", how = \"inner\")\n",
    "df_cap = pd.merge(df_countyFIPSall, df_cap, on = \"CountyFIPS\", how = \"inner\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7e60a7-d64f-4e44-9529-0c71004df56b",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd6c0085-401e-478f-ab6c-ae23e3f5baac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Climate (capped) counties and variables =  (2082, 4)\n",
      "--Social Vulnerability (capped) counties and variables =  (2082, 4)\n",
      "--Political landscape (capped) counties and variables =  (2082, 4)\n",
      "--Labor market (capped) counties and variables =  (2082, 4)\n",
      "++Variance explained by 3 principal components = 99.8 %\n",
      "++Variance explained by 3 principal components = 94.56 %\n",
      "++Variance explained by 3 principal components = 96.96 %\n",
      "++Variance explained by 3 principal components = 99.61 %\n",
      "--Climate Weights:   [0.33320798 0.16899246 0.33325876 0.1645408 ]\n",
      "--SV Weights:        [0.22030184 0.33321995 0.31069624 0.13578197]\n",
      "--Pol Weights:       [0.27644834 0.33271935 0.27237171 0.11846061]\n",
      "--Labor Weights:     [0.22726215 0.23167207 0.27259732 0.26846846]\n"
     ]
    }
   ],
   "source": [
    "# df filtering based on the major groups\n",
    "# 1. Climate\n",
    "df_clim = df_cap.filter(regex='clim')\n",
    "df_clim_original = df.filter(regex='clim')\n",
    "print(f\"--Climate (capped) counties and variables = \", df_clim.shape)\n",
    "\n",
    "# 2. Social Vulnerability\n",
    "df_sv = df_cap.filter(regex='sv')\n",
    "df_sv_original = df.filter(regex='sv')\n",
    "print(f\"--Social Vulnerability (capped) counties and variables = \", df_sv.shape)\n",
    "\n",
    "# 3. Political Willingness\n",
    "df_pol = df_cap.filter(regex='pol')\n",
    "df_pol_original = df.filter(regex='pol')\n",
    "print(f\"--Political landscape (capped) counties and variables = \", df_pol.shape)\n",
    "\n",
    "# 4. LMI\n",
    "df_lm = df_cap.filter(regex='lm')\n",
    "df_lm_original = df.filter(regex='lm')\n",
    "print(f\"--Labor market (capped) counties and variables = \", df_lm.shape)\n",
    "\n",
    "# Create a list of your datasets (replace these with your actual data)\n",
    "datasets = [df_clim, df_sv, df_pol, df_lm]\n",
    "\n",
    "# Create an empty list to store results\n",
    "results = []\n",
    "\n",
    "# Loop through each dataset\n",
    "for data in datasets:\n",
    "    # Assuming you have your data in X\n",
    "    X = data.values\n",
    "\n",
    "    # Set the number of components you want to retain\n",
    "    num_components = 3  # Adjust as needed\n",
    "\n",
    "    # Standardize the data using StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    X_standardized = scaler.fit_transform(X)\n",
    "\n",
    "    # Perform PCA on the standardized data\n",
    "    pca = PCA(n_components=num_components)\n",
    "    pca.fit(X_standardized)\n",
    "    print(\"++Variance explained by 3 principal components =\",\n",
    "          np.cumsum(pca.explained_variance_ratio_ * 100)[2].round(2), \"%\")\n",
    "\n",
    "    # Get the loadings matrix\n",
    "    loadings = pca.components_\n",
    "\n",
    "    # Calculate the variance explained by each variable\n",
    "    variance_explained_by_variable = np.sum(loadings**2, axis=0)\n",
    "\n",
    "    # Normalize the variances. This normalization ensures that the weights will collectively sum to 1.\n",
    "    normalized_variances = variance_explained_by_variable / np.sum(variance_explained_by_variable)\n",
    "\n",
    "    # Append the normalized variances to the results list\n",
    "    results.append(normalized_variances) # Each row represents the normalized variances for one dataset's variables.\n",
    "\n",
    "weights_clim = results[0]\n",
    "weights_sv = results[1]\n",
    "weights_pol = results[2]\n",
    "weights_lm = results[3]\n",
    "\n",
    "print(\"--Climate Weights:  \", weights_clim)\n",
    "print(\"--SV Weights:       \", weights_sv)\n",
    "print(\"--Pol Weights:      \", weights_pol)\n",
    "print(\"--Labor Weights:    \", weights_lm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef51e091-f5f4-41b6-94dc-f119555de06e",
   "metadata": {},
   "source": [
    "## Standardise and Assign weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de192b6d-0296-4b8b-bf15-0d9af58da458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.478328\n",
      "1       0.456582\n",
      "2       0.490540\n",
      "3       0.428817\n",
      "4       0.463513\n",
      "          ...   \n",
      "3131    0.443766\n",
      "3132    0.469308\n",
      "3133    0.443766\n",
      "3134    0.285849\n",
      "3135    0.443766\n",
      "Name: lm_demand_growth, Length: 3136, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "datasets = [df_clim, df_sv, df_pol, df_lm]\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Apply custom Min-Max scaling to columns 2 and 3 of dataset4 and store the results\n",
    "custom_scaled_lmindex2 = (df_lm_original.iloc[:, 2] - df_lm_original.iloc[:, 2].min()) / (df_lm_original.iloc[:, 2].max() - df_lm_original.iloc[:, 2].min())\n",
    "custom_scaled_lmindex3 = (df_lm_original.iloc[:, 3] - df_lm_original.iloc[:, 3].min()) / (df_lm_original.iloc[:, 3].max() - df_lm_original.iloc[:, 3].min())\n",
    "\n",
    "# Combine the datasets\n",
    "combined_data = pd.concat([df_clim_original, df_sv_original, df_pol_original, df_lm_original], axis = 1)\n",
    "\n",
    "# Apply Min-Max scaling to the entire dataset\n",
    "scaled_data = scaler.fit_transform(combined_data)\n",
    "\n",
    "# Convert the scaled_data NumPy array to a DataFrame\n",
    "scaled_data_df = pd.DataFrame(scaled_data, columns=combined_data.columns)\n",
    "print(custom_scaled_lmindex2)\n",
    "# Replace the last two columns of scaled_data with the custom scaled values\n",
    "scaled_data_df.loc[:, scaled_data_df.columns[-2]] = custom_scaled_lmindex2\n",
    "scaled_data_df.loc[:, scaled_data_df.columns[-1]] = custom_scaled_lmindex3\n",
    "\n",
    "# Create DataFrames for the scaled data\n",
    "df_clim_scaled = scaled_data_df.iloc[:, :4]\n",
    "df_sv_scaled = scaled_data_df.iloc[:, 4:8]\n",
    "df_pol_scaled = scaled_data_df.iloc[:, 8:12]\n",
    "df_lm_scaled = scaled_data_df.iloc[:, 12:]\n",
    "\n",
    "### WEIGHTS\n",
    "# Create a list of dataset names or identifiers\n",
    "datasets = [df_clim_scaled, df_sv_scaled, df_pol_scaled, df_lm_scaled]\n",
    "\n",
    "# Create an empty dictionary to store the weighted averages\n",
    "weighted_averages_dict = {}\n",
    "\n",
    "# Loop through the datasets and their corresponding PCA results\n",
    "for dataset, result in zip(datasets, results):\n",
    "      # Calculate the weighted average using the result for this specific dataset\n",
    "      weighted_average = np.dot(dataset.values, result)\n",
    "\n",
    "      # Store the weighted average in the dictionary with a dataset identifier\n",
    "      dataset_name = f'Dataset {len(weighted_averages_dict) + 1}'  # You can customize the naming\n",
    "      weighted_averages_dict[dataset_name] = weighted_average\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "weighted_averages_df = pd.DataFrame(weighted_averages_dict)\n",
    "weighted_averages_df.columns = ['clim_w', 'sv_w', 'pol_w', 'lm_w']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c73dc3d-2e67-4e6b-87c2-e2f157de3d13",
   "metadata": {},
   "source": [
    "## Risk/Readiness Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "024bb7a2-3ae7-4ca4-9db2-f2a11ab23d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "risk = (weighted_averages_df['clim_w'] + weighted_averages_df['sv_w']) / 2\n",
    "readiness = (weighted_averages_df['lm_w'] + weighted_averages_df['pol_w']) / 2\n",
    "\n",
    "df_final = pd.concat([risk, readiness], axis = 1)\n",
    "# Multiply every element by 100 and round to 2 decimal points for legibility\n",
    "df_final *= 100\n",
    "df_final = df_final.round(2)\n",
    "df_final = pd.concat([df.iloc[:, 0:2], df_final], axis = 1) #add fips and county name\n",
    "df_final.columns = ['fips', 'county', 'risk', 'readiness']\n",
    "df_final['fips'] = df_final['fips'].astype(str).apply(lambda x: x.zfill(5) if len(x) == 4 else x) #need the leading zeros for geojson\n",
    "\n",
    "#calculate the medians\n",
    "risk_median = df_final['risk'].median()\n",
    "ready_median = df_final['readiness'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c051931-90c0-4b44-a46f-e1ebdc5db18a",
   "metadata": {},
   "source": [
    "## Calculate and Assign Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9e472c5-0632-4c13-8e0f-e02cd27c8ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Number of counties that are 'critical': 978\n",
      "--Number of counties that are 'early': 978\n",
      "--Number of counties that are 'exposed': 590\n",
      "--Number of counties that are 'primed': 590\n",
      "--risk_type\n",
      "Climate Greater    2591\n",
      "SV Greater          545\n",
      "Name: count, dtype: int64\n",
      "--ready_type\n",
      "Policy Greater    1943\n",
      "LM Greater        1193\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Custom function to apply the 'Persona' conditions\n",
    "def assign_category(row):\n",
    "    if row['readiness'] >= ready_median:\n",
    "        if row['risk'] >= risk_median:\n",
    "            return 'critical'\n",
    "        else:\n",
    "            return 'primed'\n",
    "    else:\n",
    "        if row['risk'] >= risk_median:\n",
    "            return 'exposed'\n",
    "        else:\n",
    "            return 'early'\n",
    "\n",
    "# Create a new column 'Category' based on the conditions\n",
    "df_final['category'] = df_final.apply(assign_category, axis=1)\n",
    "\n",
    "# Get the value counts\n",
    "value_counts = df_final['category'].value_counts()\n",
    "\n",
    "# Loop through the values and print with \"--\" prefix\n",
    "for category, count in value_counts.items():\n",
    "    print(f\"--Number of counties that are '{category}': {count}\")\n",
    "\n",
    "\n",
    "# First Comparison\n",
    "def compare_rows_risk(row):\n",
    "    if row['clim_w'] >= row['sv_w']:\n",
    "        return 'Climate Greater'\n",
    "    else:\n",
    "        return 'SV Greater'\n",
    "\n",
    "# Apply the first comparison row-wise and create the new column 'comparison_result_1' in 'new_df'\n",
    "df_final['risk_type'] = weighted_averages_df.apply(compare_rows_risk, axis=1)\n",
    "\n",
    "# Second Comparison\n",
    "def compare_rows_readiness(row):\n",
    "    if row['pol_w'] >= row['lm_w']:\n",
    "        return 'Policy Greater'\n",
    "    else:\n",
    "        return 'LM Greater'\n",
    "\n",
    "# Apply the second comparison row-wise and create the new column 'comparison_result_2' in 'new_df'\n",
    "df_final['ready_type'] = weighted_averages_df.apply(compare_rows_readiness, axis=1)\n",
    "print(f\"--{df_final['risk_type'].value_counts()}\")\n",
    "print(f\"--{df_final['ready_type'].value_counts()}\")\n",
    "\n",
    "weighted_averages_df *= 100\n",
    "weighted_averages_df = weighted_averages_df.round(2)\n",
    "weighted_averages_df = pd.concat([df.iloc[:, 0:2], weighted_averages_df], axis = 1) #add fips and county name\n",
    "weighted_averages_df.columns = ['fips', 'county', 'climate', 'social', 'political', 'labor']\n",
    "weighted_averages_df['fips'] = weighted_averages_df['fips'].astype(str).apply(lambda x: x.zfill(5) if len(x) == 4 else x) #need the leading zeros for geojson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3addb271-7800-4314-89b4-ea431fecccb8",
   "metadata": {},
   "source": [
    "## Export to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "03c9dab8-83be-401f-9f0c-83032441cfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_riskready = '/Users/aconway/Desktop/CREST/Crest_Data/finaldataset.csv'\n",
    "df_final.to_csv(filepath_riskready, index=False)\n",
    "\n",
    "filepath_layers = '/Users/aconway/Desktop/CREST/Crest_Data/finallayers.csv'\n",
    "weighted_averages_df.to_csv(filepath_layers, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
